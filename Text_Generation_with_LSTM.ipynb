{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JKinAPkGVlKN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3844,
     "status": "ok",
     "timestamp": 1605839130782,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "Wv6NkeinVuuI",
    "outputId": "f921c7d2-d778-4861-d04b-0a9c80f28152"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 3830,
     "status": "ok",
     "timestamp": 1605839130784,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "teF3tVFYVwOH",
    "outputId": "8d209a8d-82f7-4806-cee1-c4e3fe3f653d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.get_device_name(0)\n",
    "else:\n",
    "    print(\"No GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA9ibcezXKCR"
   },
   "source": [
    "# **Part 1 - Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AZFlDMRrWAm0"
   },
   "outputs": [],
   "source": [
    "#Opening Pride & Prejudice \n",
    "with open('pride_and_prejudice.txt', 'r', encoding = 'utf8') as f:\n",
    "#code 'r' stands for reading, encoding 'utf8' passes in the file as a string, and save this all as variable, f\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 6040,
     "status": "ok",
     "timestamp": 1605839133013,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "l_ciOfX6W1Ob",
    "outputId": "7207fce6-64f4-494b-a5b7-58d6e537eb9e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nIt is a truth universally acknowledged, that a single man in possession\\nof a good fortune, must be in want of a wife.\\n\\nHowever little known the feelings or views of such a man may be on his\\nfirst entering a neighbourhood, this truth is so well fixed in the minds\\nof the surrounding families, that he is considered the rightful property\\nof some one or other of their daughters.\\n\\n“My dear Mr. Bennet,” said his lady to him one day, “have you heard that\\nNetherfield Park is let at last?”\\n\\nMr. Bennet replied that he had not.\\n\\n“But it is,” returned she; “for Mrs. Long has just been here, and she\\ntold me all about it.”\\n\\nMr. Bennet made no answer.\\n\\n“Do you not want to know who has taken it?” cried his wife impatiently.\\n\\n“_You_ want to tell me, and I have no objection to hearing it.”\\n\\nThis was invitation enough.\\n\\n“Why, my dear, you must know, Mrs. Long says that Netherfield is taken\\nby a young man of large fortune from the north of England; that he came\\ndown on Monday in a chaise and fo'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000] #\\n stands for new line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6026,
     "status": "ok",
     "timestamp": 1605839133016,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "1O4JKYRhXBvW",
    "outputId": "b637e364-4d6e-490c-8205-18a35ef105a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "684743"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text) #so we got 400k characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_x5U3aN5XOXg"
   },
   "source": [
    "# **Part 2 - Encoding the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6175,
     "status": "ok",
     "timestamp": 1605839133174,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "L3lJc3LmXFTF",
    "outputId": "7b4b97cd-3ba6-4be1-c2f1-54459bc50e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L', '6', 'x', 'k', 'p', 'M', 'C', 'z', 'n', 'E', '”', 'Y', '!', 'F', '(', '.', 'D', 'e', 'N', 'H', 'R', '4', '8', 'd', '?', 'Z', 'r', \"'\", 'O', 't', 'W', '\\n', 'S', 'G', ':', ',', 'b', 'f', 'A', 'm', 'J', 'V', '3', '7', 'y', 'g', '*', '0', '1', 's', 'K', 'i', '9', 'o', 'h', 'v', 'w', '5', '2', ';', '“', 'u', 'T', 'B', '_', ')', 'q', 'c', 'U', ' ', '-', 'j', 'a', 'I', 'l', 'P'}\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "all_characters = set(text) #now we're separating each individual character in this text set\n",
    "print(all_characters) #it returned all the unique characters\n",
    "print(len(all_characters)) #84 unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6169,
     "status": "ok",
     "timestamp": 1605839133175,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "R6XgPOPrXdVL",
    "outputId": "223b4c7a-a165-4e84-8a20-55d438a6de7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'L', 1: '6', 2: 'x', 3: 'k', 4: 'p', 5: 'M', 6: 'C', 7: 'z', 8: 'n', 9: 'E', 10: '”', 11: 'Y', 12: '!', 13: 'F', 14: '(', 15: '.', 16: 'D', 17: 'e', 18: 'N', 19: 'H', 20: 'R', 21: '4', 22: '8', 23: 'd', 24: '?', 25: 'Z', 26: 'r', 27: \"'\", 28: 'O', 29: 't', 30: 'W', 31: '\\n', 32: 'S', 33: 'G', 34: ':', 35: ',', 36: 'b', 37: 'f', 38: 'A', 39: 'm', 40: 'J', 41: 'V', 42: '3', 43: '7', 44: 'y', 45: 'g', 46: '*', 47: '0', 48: '1', 49: 's', 50: 'K', 51: 'i', 52: '9', 53: 'o', 54: 'h', 55: 'v', 56: 'w', 57: '5', 58: '2', 59: ';', 60: '“', 61: 'u', 62: 'T', 63: 'B', 64: '_', 65: ')', 66: 'q', 67: 'c', 68: 'U', 69: ' ', 70: '-', 71: 'j', 72: 'a', 73: 'I', 74: 'l', 75: 'P'}\n"
     ]
    }
   ],
   "source": [
    "#So we're going to create 2 objects, an encoder and de-coder\n",
    "#An encoder will take a character/object and return it's encoded value\n",
    "#A decoder will take a value, and return it's corresponding character/object\n",
    "decoder = dict(enumerate(all_characters))#So all we did was create a dictionary by enumerating all the characters to get number value -- > characer\n",
    "print(decoder) #see the dictionary now has a corresponding value for each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6162,
     "status": "ok",
     "timestamp": 1605839133176,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "rzPLwRwsYSMv",
    "outputId": "406ad6ac-fbb7-4464-ec49-fd0f85d0d0c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'L': 0, '6': 1, 'x': 2, 'k': 3, 'p': 4, 'M': 5, 'C': 6, 'z': 7, 'n': 8, 'E': 9, '”': 10, 'Y': 11, '!': 12, 'F': 13, '(': 14, '.': 15, 'D': 16, 'e': 17, 'N': 18, 'H': 19, 'R': 20, '4': 21, '8': 22, 'd': 23, '?': 24, 'Z': 25, 'r': 26, \"'\": 27, 'O': 28, 't': 29, 'W': 30, '\\n': 31, 'S': 32, 'G': 33, ':': 34, ',': 35, 'b': 36, 'f': 37, 'A': 38, 'm': 39, 'J': 40, 'V': 41, '3': 42, '7': 43, 'y': 44, 'g': 45, '*': 46, '0': 47, '1': 48, 's': 49, 'K': 50, 'i': 51, '9': 52, 'o': 53, 'h': 54, 'v': 55, 'w': 56, '5': 57, '2': 58, ';': 59, '“': 60, 'u': 61, 'T': 62, 'B': 63, '_': 64, ')': 65, 'q': 66, 'c': 67, 'U': 68, ' ': 69, '-': 70, 'j': 71, 'a': 72, 'I': 73, 'l': 74, 'P': 75}\n"
     ]
    }
   ],
   "source": [
    "encoder = {char: ind for ind, char in decoder.items()} #for index and character in decoder, since each dictionary item is an index + character\n",
    "print(encoder) #notice the values match up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6935,
     "status": "ok",
     "timestamp": 1605839133956,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "cOBBIsAFYjur",
    "outputId": "6b4b89b6-5312-4625-9911-493c71e1b863"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 54, 72,  4, 29, 17, 26, 69, 48, 31, 31, 31, 73, 29, 69, 51, 49,\n",
       "       69, 72, 69, 29, 26, 61, 29, 54, 69, 61,  8, 51, 55, 17, 26, 49, 72,\n",
       "       74, 74, 44, 69, 72, 67,  3,  8, 53, 56, 74, 17, 23, 45, 17, 23, 35,\n",
       "       69, 29, 54, 72, 29, 69, 72, 69, 49, 51,  8, 45, 74, 17, 69, 39, 72,\n",
       "        8, 69, 51,  8, 69,  4, 53, 49, 49, 17, 49, 49, 51, 53,  8, 31, 53,\n",
       "       37, 69, 72, 69, 45, 53, 53, 23, 69, 37, 53, 26, 29, 61,  8, 17, 35,\n",
       "       69, 39, 61, 49, 29, 69, 36, 17, 69, 51,  8, 69, 56, 72,  8, 29, 69,\n",
       "       53, 37, 69, 72, 69, 56, 51, 37, 17, 15, 31, 31, 19, 53, 56, 17, 55,\n",
       "       17, 26, 69, 74, 51, 29, 29, 74, 17, 69,  3,  8, 53, 56,  8, 69, 29,\n",
       "       54, 17, 69, 37, 17, 17, 74, 51,  8, 45, 49, 69, 53, 26, 69, 55, 51,\n",
       "       17, 56, 49, 69, 53, 37, 69, 49, 61, 67, 54, 69, 72, 69, 39, 72,  8,\n",
       "       69, 39, 72, 44, 69, 36, 17, 69, 53,  8, 69, 54, 51])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text = np.array([encoder[char] for char in text]) #creating a numpy array for each character in text with it's corresponding encoded value\n",
    "encoded_text[:200] #notice how each character is replaced by its corresponding encoded value now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6931,
     "status": "ok",
     "timestamp": 1605839133965,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "Qs5242NUZdVZ",
    "outputId": "737a869e-0fc3-4935-fde9-6e324f536a84"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'H'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[19] #see how 19 = H based on our dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0Ck58ucAZmTj"
   },
   "outputs": [],
   "source": [
    "#Now applying a one hot encoding to the data\n",
    "def one_hot_encoder(encoded_text, num_unique_chars):\n",
    "  #encoded_text will be a batch of encoded text, so not all of the text data at once\n",
    "  #num_unique_chars is len(set(text)), which is the sum of how many unique characters that we have. But this is for the batch of text, unlike what we did above for all the data at once\n",
    "  one_hot = np.zeros((encoded_text.size, num_unique_chars)) #intiailizing our one hot encoding matrix with all zeros and dimensions size of encoded data by num of unique characters\n",
    "  one_hot = one_hot.astype(np.float32) #making sure it's a float type\n",
    "\n",
    "  one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0 #so here we're basically taking the shape of one_hot and flattening the encoded text, and where-ever the index position aligns, set it to 1. So basically 'L' = 0, will now be [1,0,0..0] and 'x' = 2, will now be [0,0,1..0]\n",
    "  #read this https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array\n",
    "\n",
    "  one_hot = one_hot.reshape((*encoded_text.shape,num_unique_chars)) #so reshaping our one hot to match the actual batch shape\n",
    "\n",
    "  return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6925,
     "status": "ok",
     "timestamp": 1605839133968,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "DrPT5ifBcv10",
    "outputId": "56d6a33b-6ca1-4763-9622-54eddc413d19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#testing out our function with an example array\n",
    "example_arr = np.array([1,2,0])\n",
    "one_hot_encoder(example_arr, 3) #see how they're all encoded now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfUUM1zUdpQg"
   },
   "source": [
    "# **Part 3 - Generating Training Batches**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6918,
     "status": "ok",
     "timestamp": 1605839133969,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "SNywHa5Pc47A",
    "outputId": "e3f258e8-e284-477d-a75c-05233711e93b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = np.arange(10)\n",
    "example_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6914,
     "status": "ok",
     "timestamp": 1605839133971,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "QJsu3bNSkZhp",
    "outputId": "ca1aff41-dff8-47df-d608-13578176bc43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5],\n",
       "       [6, 7],\n",
       "       [8, 9]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text.reshape((5,-1)) #reshaping into samples per batch and 2 columns inferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "8dFaiN-xk1mF"
   },
   "outputs": [],
   "source": [
    "#Creating a function to generate batches\n",
    "def generate_batches(encoded_text, samples_per_batch=10, seq_len=50):\n",
    "  # X: encoded text of length seq_len\n",
    "  # X_example = ([0,1,2],\n",
    "  #              [1,2,3]) so we have 2 samples per batch with sequence length of 3 (per batch). Giving us 6 char_per_batch \n",
    "\n",
    "\n",
    "  # Y: encoded text shifted by one\n",
    "  # Y_example = ([1,2,3],\n",
    "  #              [2,3,4])\n",
    "  char_per_batch = samples_per_batch * seq_len #total number of characters being used per batch\n",
    "\n",
    "  num_batches_avail = int(len(encoded_text)/char_per_batch) #so finding how many batches tota by dividing the total length of encoded text by the number of characters per batch\n",
    "\n",
    "  encoded_text = encoded_text[:num_batches_avail*char_per_batch] #so now we're just cutting off the end of the encoded text that won't fit evenly into a batch\n",
    "\n",
    "  encoded_text = encoded_text.reshape((samples_per_batch, -1)) #now just reshaping it to rows = samples per batch, columns inferred to be the seq_len like we did with example text above\n",
    "  #if encoded_text length is 40, and samples per batch is 2 and seq length of 4, then this will be shape (2,4) for now\n",
    "\n",
    "  for n in range(0,encoded_text.shape[1], seq_len):\n",
    "    #starting from 0 to column size of encoded_text with a step size of seq_len\n",
    "    x = encoded_text[:,n:n+seq_len] #grab all the samples per batch (rows) at column n to n+seq_len (i.e. 0 to 4 if seq len is 4)\n",
    "    y = np.zeros_like(x) #creating a zeros array with same shape as x\n",
    "\n",
    "    try:\n",
    "      y[:,:-1] = x[:,1:] #so we're making every row and every column till the last in y basically equal every sample in X starting from the second character till the end\n",
    "      #y[1,2,3] = x[0,1,2,3]\n",
    "      y[:,-1] = encoded_text[:, n+seq_len] #now we're assigning every row and the last column in y equal to n+seq_len, which will just be shifted over by 1\n",
    "      #y[1,2,3,4] = encoded_text[0+4 = 4]\n",
    "    except:\n",
    "      #this is incase we get an out of range indexing error for the very end of our encoded text\n",
    "      y[:,:-1] = x[:,1:]\n",
    "      y[:,-1] = encoded_text[:,0]\n",
    "\n",
    "    yield x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6904,
     "status": "ok",
     "timestamp": 1605839133973,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "8vU-8n98sKjs",
    "outputId": "78e15f58-073b-46f6-b4ae-1253df9acbb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6, 54, 72,  4, 29, 17, 26, 69, 48, 31, 31, 31, 73, 29, 69, 51, 49,\n",
       "       69, 72, 69, 29, 26, 61, 29, 54, 69, 61,  8, 51, 55, 17, 26, 49, 72,\n",
       "       74, 74, 44, 69, 72, 67])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = encoded_text[:40]\n",
    "sample_text\n",
    "#len(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7008,
     "status": "ok",
     "timestamp": 1605839134084,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "zL8baxA3sPcq",
    "outputId": "829bea72-8823-4ffa-be12-bc62f11e4dc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6, 54, 72,  4],\n",
       "       [29, 26, 61, 29]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_generator = generate_batches(sample_text, samples_per_batch = 2, seq_len=4)\n",
    "\n",
    "x, y = next(batch_generator)\n",
    "x #so made 2 samples (rows) with a seq len of 4 (columns), but also we cut the remaining 16 characters are since they didn't feed into our size\n",
    "#the second row, [29,26,61,29] starts at character 21, since encoded text length is 40 and 2 samples per batch means each sample can be up to 20 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7001,
     "status": "ok",
     "timestamp": 1605839134085,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "keOST6olsbMm",
    "outputId": "e85a42d4-0a9c-4fd9-9be5-232ed26e1a08"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[54, 72,  4, 29],\n",
       "       [26, 61, 29, 54]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y #notice it's shifted over by 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b13fdvhsDCl9"
   },
   "source": [
    "# **Part 4: Creating our LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "swero1Q4svGB"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "  \n",
    "  def __init__(self, all_chars, num_hidden = 256, num_layers = 4, dropout_prob = 0.5, use_gpu = False):\n",
    "\n",
    "    super().__init__() #super is a function that allows us to use Module's functions and variables, while also optimizing this inheritance process\n",
    "    \n",
    "    #Just intializing our attributes\n",
    "    self.dropout_prob = dropout_prob\n",
    "    self.num_hidden = num_hidden\n",
    "    self.num_layers = num_layers\n",
    "    self.use_gpu = use_gpu\n",
    "    self.all_chars = all_chars\n",
    "    self.decoder = dict(enumerate(all_chars))#So all we did was create a dictionary by enumerating all the characters to get number value assigned to each character\n",
    "    self.encoder = {char: ind for ind, char in decoder.items()} #for index and character in decoder, since each dictionary item is an index + character\n",
    "\n",
    "    #Adding LSTM layer\n",
    "    self.lstm = nn.LSTM(input_size = len(self.all_chars), hidden_size = num_hidden, num_layers = num_layers, dropout = dropout_prob, batch_first = True,) #so here we're creating an LSTM where input size is one feature per unique character. Dropout here is for LSTM specifically\n",
    "    self.dropout = nn.Dropout(dropout_prob) #drop out here is to go into the FC1\n",
    "    #Adding fully connected layer\n",
    "    self.fc_linear = nn.Linear(in_features = num_hidden, out_features = len(self.all_chars)) #now we're creating a fully connected layer that takes the hidden LSTM layers and can output to all unique characters\n",
    "\n",
    "\n",
    "  def forward(self, x, hidden):\n",
    "    lstm_out , hidden = self.lstm(x, hidden) #so our lstm expects inputs as: input, (h_0, c_0) = (seq_len, batch, input_size), (h_0, c_0)\n",
    "    #so our first tuple contains len of the sequence, batch size, and input size. second tuple contains the hidden state and cell state\n",
    "    #lstm outputs: output, (h_n, c_n) = (seq_len, batch, num_directions * hidden_size), (h_n, c_n), so we're doing tuple unpacking to update our hidden_state and creating a new variable for the for the output\n",
    "    drop_output = self.dropout(lstm_out)\n",
    "    drop_output = drop_output.contiguous().view(-1,self.num_hidden) #reshaping this dropout layer so that we can connect it to our output layer\n",
    "\n",
    "    final_out = self.fc_linear(drop_output)\n",
    "    return final_out, hidden\n",
    "\n",
    "  def hidden_state(self, batch_size):\n",
    "    if self.use_gpu:\n",
    "      #Initializing h0 and c0 for cuda\n",
    "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
    "                #so recall the LSTM takes in 3 inputs: x(t), h(t-1), c(t-1), so we're passing 3 tensors into the LSTM layer\n",
    "                #we are using zeros to basically initialize this hidden state, h(t-1), and cell state, c(t-1)\n",
    "                #so we're creating a num_layers x batch_size x num_hidden sized matrix with only zeros\n",
    "    else:\n",
    "      #Initializing h0 and c0 for CPU\n",
    "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
    "                torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "FM1LXgXLI9is"
   },
   "outputs": [],
   "source": [
    "model = LSTM(all_characters, num_hidden = 300, num_layers = 2, dropout_prob = 0.5, use_gpu = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6989,
     "status": "ok",
     "timestamp": 1605839134086,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "2_-z9fT3JJrb",
    "outputId": "789ed3c9-5fc8-4d48-e428-f33d2693f7e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1198876"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_param = []\n",
    "\n",
    "for p in model.parameters():\n",
    "  total_param.append(int(p.numel()))\n",
    "\n",
    "sum(total_param) #so notice how the number of parameters is very close to the total number of characters. This is a good way to start adjusting the model parameters so that we can avoid overfitting (too many parameters) or underfitting (too few parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7096,
     "status": "ok",
     "timestamp": 1605839134200,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "a6syPDhqJgRX",
    "outputId": "5ca96f20-2629-4d24-9523-59b2831e6019"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of LSTM(\n",
       "  (lstm): LSTM(76, 300, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=300, out_features=76, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss() #so our loss measurement will be Cross Entropy Loss since this is a categorical problem\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) #model parameters are just the fully connected layers and we are using Adam optimizer to optimize them\n",
    "model.parameters #can see the parameters are just the fully connected layers and we are optimizing them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "H49iRCCNKEJk"
   },
   "outputs": [],
   "source": [
    "train_percent = 0.9 #use 90% of our data for training\n",
    "train_index = int(len(encoded_text) * train_percent) #making our training index 90% of the length of the encoded text\n",
    "\n",
    "train_data = encoded_text[:train_index] #so make our training data all the data from the start till the train_index in encoded text\n",
    "val_data = encoded_text[train_index:] #making our validation from the train_index to the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h2sgHg5ZGzCS"
   },
   "source": [
    "# **Part 5 - Training our Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010190,
     "status": "ok",
     "timestamp": 1605840538036,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "XH5BZlceKGMf",
    "outputId": "03096617-2026-432a-8e23-c0bedc77a293"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0    Step: 50     Val Loss: 3.12415481\n",
      "Epoch: 1    Step: 100     Val Loss: 3.10654974\n",
      "Epoch: 2    Step: 150     Val Loss: 2.93200564\n",
      "Epoch: 3    Step: 200     Val Loss: 2.64234090\n",
      "Epoch: 4    Step: 250     Val Loss: 2.51444983\n",
      "Epoch: 4    Step: 300     Val Loss: 2.41361380\n",
      "Epoch: 5    Step: 350     Val Loss: 2.31937313\n",
      "Epoch: 6    Step: 400     Val Loss: 2.24604893\n",
      "Epoch: 7    Step: 450     Val Loss: 2.16895008\n",
      "Epoch: 8    Step: 500     Val Loss: 2.10609818\n",
      "Epoch: 9    Step: 550     Val Loss: 2.04523754\n",
      "Epoch: 9    Step: 600     Val Loss: 1.99152970\n",
      "Epoch: 10    Step: 650     Val Loss: 1.94423401\n",
      "Epoch: 11    Step: 700     Val Loss: 1.90591168\n",
      "Epoch: 12    Step: 750     Val Loss: 1.86582065\n",
      "Epoch: 13    Step: 800     Val Loss: 1.83111167\n",
      "Epoch: 13    Step: 850     Val Loss: 1.78864145\n",
      "Epoch: 14    Step: 900     Val Loss: 1.76076841\n",
      "Epoch: 15    Step: 950     Val Loss: 1.72695374\n",
      "Epoch: 16    Step: 1000     Val Loss: 1.70063770\n",
      "Epoch: 17    Step: 1050     Val Loss: 1.67548692\n",
      "Epoch: 18    Step: 1100     Val Loss: 1.65146470\n",
      "Epoch: 18    Step: 1150     Val Loss: 1.62436283\n",
      "Epoch: 19    Step: 1200     Val Loss: 1.59965611\n",
      "Epoch: 20    Step: 1250     Val Loss: 1.58072972\n",
      "Epoch: 21    Step: 1300     Val Loss: 1.56975532\n",
      "Epoch: 22    Step: 1350     Val Loss: 1.54895687\n",
      "Epoch: 22    Step: 1400     Val Loss: 1.53492844\n",
      "Epoch: 23    Step: 1450     Val Loss: 1.51880741\n",
      "Epoch: 24    Step: 1500     Val Loss: 1.50157404\n",
      "Epoch: 25    Step: 1550     Val Loss: 1.48477864\n",
      "Epoch: 26    Step: 1600     Val Loss: 1.47177780\n",
      "Epoch: 27    Step: 1650     Val Loss: 1.46161723\n",
      "Epoch: 27    Step: 1700     Val Loss: 1.44904160\n",
      "Epoch: 28    Step: 1750     Val Loss: 1.43567181\n",
      "Epoch: 29    Step: 1800     Val Loss: 1.42408741\n",
      "Epoch: 30    Step: 1850     Val Loss: 1.41360569\n",
      "Epoch: 31    Step: 1900     Val Loss: 1.40870643\n",
      "Epoch: 31    Step: 1950     Val Loss: 1.40100527\n",
      "Epoch: 32    Step: 2000     Val Loss: 1.39032984\n",
      "Epoch: 33    Step: 2050     Val Loss: 1.38237762\n",
      "Epoch: 34    Step: 2100     Val Loss: 1.37390924\n",
      "Epoch: 35    Step: 2150     Val Loss: 1.36351919\n",
      "Epoch: 36    Step: 2200     Val Loss: 1.35851538\n",
      "Epoch: 36    Step: 2250     Val Loss: 1.35214245\n",
      "Epoch: 37    Step: 2300     Val Loss: 1.34098279\n",
      "Epoch: 38    Step: 2350     Val Loss: 1.33749938\n",
      "Epoch: 39    Step: 2400     Val Loss: 1.33019149\n",
      "Training took 102.25820562839507 minutes\n"
     ]
    }
   ],
   "source": [
    "import time #Gonna try to keep track of how long it takes to train our model\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "epochs = 40\n",
    "batch_size = 100\n",
    "seq_len = 100\n",
    "tracker = 0\n",
    "num_char = max(encoded_text)+1\n",
    "\n",
    "model.train()\n",
    "\n",
    "if model.use_gpu:\n",
    "  model.cuda()\n",
    "\n",
    "#This for loop trains our NN\n",
    "for i in range(epochs):\n",
    "  #Forward propagation through our ANN using training data\n",
    "  hidden = model.hidden_state(batch_size)\n",
    "\n",
    "  for x,y in generate_batches(encoded_text = train_data, samples_per_batch = batch_size, seq_len = seq_len):\n",
    "    tracker += 1\n",
    "    #recall generate_batches returns x (training) and y (label)\n",
    "    x = one_hot_encoder(encoded_text = x, num_unique_chars = num_char) #applying one hot encoding to our training data\n",
    "    \n",
    "    inputs = torch.from_numpy(x) #converting training data into a torch tensor\n",
    "    targets = torch.from_numpy(y) #converting labels into a torch tensor\n",
    "\n",
    "    if model.use_gpu:\n",
    "      inputs = inputs.cuda()\n",
    "      targets = targets.cuda()\n",
    "    \n",
    "    hidden = tuple([state.data for state in hidden]) #here we're grabbing the hidden state, so that we can reset it in the next line (otherwise it would backpropagate through all the training history)\n",
    "    model.zero_grad() #resetting the gradient on the model so the hidden state doesn't accumulate\n",
    "\n",
    "    lstm_output, hidden = model.forward(inputs, hidden) #recall our forward returns the final output and hidden\n",
    "\n",
    "    #Calculating loss/error\n",
    "    loss = criterion(lstm_output, targets.view(batch_size*seq_len).long()) #reshaping our targets into the correct format to compare to our predictions, lstm outputs \n",
    "\n",
    "    #Backpropagation\n",
    "    loss.backward() #doing backpropagation off the loss function\n",
    "\n",
    "    #Clipping off to avoid exploding gradient problem\n",
    "    nn.utils.clip_grad_norm_(model.parameters(), max_norm = 5) #capping it out at 5 to alleviate exploding gradient\n",
    "    optimizer.step() #using the optimizer for the back propagation\n",
    "\n",
    "    #tracking validation data\n",
    "    if tracker % 50 == 0:\n",
    "      val_hidden = model.hidden_state(batch_size)\n",
    "      val_losses = []\n",
    "      model.eval()\n",
    "\n",
    "      for x,y in generate_batches(encoded_text = val_data, samples_per_batch =  batch_size, seq_len = seq_len): \n",
    "        #recall generate_batches returns x (training) and y (label)\n",
    "        x = one_hot_encoder(encoded_text = x, num_unique_chars = num_char) #applying one hot encoding to our training data\n",
    "        \n",
    "        inputs = torch.from_numpy(x) #converting training data into a torch tensor\n",
    "        targets = torch.from_numpy(y) #converting labels into a torch tensor\n",
    "\n",
    "        if model.use_gpu:\n",
    "          inputs = inputs.cuda()\n",
    "          targets = targets.cuda()\n",
    "\n",
    "        val_hidden = tuple([state.data for state in val_hidden]) #here we're resetting the hidden state, otherwise it would backpropagate through all the training history\n",
    "        lstm_output, val_hidden = model.forward(inputs, val_hidden) #recall our forward returns the final output and hidden\n",
    "        val_loss = criterion(lstm_output, targets.view(batch_size*seq_len).long()) #reshaping our targets into the correct format to compare to our predictions, lstm outputs \n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "      model.train()\n",
    "      print(f'Epoch: {i}    Step: {tracker}     Val Loss: {val_loss.item():10.8f}')\n",
    "\n",
    "print(f'Training took {(time.time() - start_time)/60} minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nTPAxKJNOwDw"
   },
   "outputs": [],
   "source": [
    "model_name = 'hidden300_layers2_Pride.net'\n",
    "torch.save(model.state_dict(),model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aA74aF30CypR"
   },
   "source": [
    "# **Part 6 - Generating Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "j9On6dOqVp3c"
   },
   "outputs": [],
   "source": [
    "#Making a function to predict the next character\n",
    "def predict_next_character(model, char, hidden = None, k = 1):\n",
    "  encoded_text = model.encoder[char]\n",
    "\n",
    "  encoded_text = np.array([[encoded_text]]) #converting it into a numpy array\n",
    "\n",
    "  encoded_text = one_hot_encoder(encoded_text = encoded_text, num_unique_chars = len(model.all_chars)) #one hot encoding our values\n",
    "\n",
    "  inputs = torch.from_numpy(encoded_text) #converting our one hot encoding numpy array into a PyTorch tensor\n",
    "\n",
    "  if (model.use_gpu):\n",
    "    inputs = inputs.cuda() #use cuda on the inputs if we're using GPU\n",
    "\n",
    "  hidden = tuple([state.data for state in hidden]) #here we're grabbing the hidden state\n",
    "\n",
    "  lstm_out, hidden = model.forward(inputs, hidden) #Now we're running our model. recall our forward returns the final output and hidden\n",
    "\n",
    "  probs = F.softmax(lstm_out, dim=1).data\n",
    "\n",
    "  if (model.use_gpu):\n",
    "    probs = probs.cpu() #moving the probabilities back to CPU so that we can use them with numpy\n",
    "\n",
    "  # k determines how many characters to consider for our probability choice.\n",
    "  # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
    "  # Return k largest probabilities in tensor\n",
    "  probs, index_positions = probs.topk(k) #tuple unpacking to get the probabilty and index position of the character\n",
    "        \n",
    "  index_positions = index_positions.numpy().squeeze() #This is why we moved back to CPU above, so that we can convert index positions into a numpy array. We did .squeeze() to get it into the right shape. np.squeeze() removes axes of size 1 (singletons).\n",
    "  #https://stackoverflow.com/questions/57237352/what-does-unsqueeze-do-in-pytorch\n",
    "  #https://numpy.org/doc/stable/reference/generated/numpy.squeeze.html\n",
    "\n",
    "  probs = probs.numpy().flatten() #flattening into a single vector array so that we can do calculations on it in the next line\n",
    "\n",
    "  probs = probs/probs.sum() \n",
    "\n",
    "  char = np.random.choice(index_positions, p=probs) #now selecting a random character of index_position and probability\n",
    "\n",
    "  return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "ReGvxD-xdW1g"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, size, seed = 'The', k = 1):\n",
    "  #k=1 means grab the top 1 most probable character. Size is how many characters we want the model to predict. Seed means we're beginning with the word 'The'.\n",
    "  if(model.use_gpu):\n",
    "    model.cuda()\n",
    "  else:\n",
    "    model.cpu()\n",
    "  \n",
    "  model.eval()\n",
    "  output_chars = [c for c in seed] #adding 'The' from seed, and will continue to append later for future characters predicted\n",
    "\n",
    "  hidden = model.hidden_state(batch_size = 1) #batch size cuz we feeding in 1 character at a time\n",
    "\n",
    "  for char in seed:\n",
    "    char, hidden = predict_next_character(model = model, char = char, hidden = hidden, k = k)\n",
    "\n",
    "  output_chars.append(char) #appending the characters that were decoded in predict_next_character\n",
    "\n",
    "  for i in range(size):\n",
    "    char, hidden = predict_next_character(model = model, char = output_chars[-1], hidden = hidden, k = k) #so now we're predicting characters based off the latest output_chars which was appended\n",
    "    output_chars.append(char)\n",
    "\n",
    "  return ''.join(output_chars) #so we're just adding the new output character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pride & Prejudice Text Generation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1605840538890,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "H8y9_3tKdb8y",
    "outputId": "2221afd2-0ccb-4587-9e07-fb35bc6df948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The she had been\n",
      "all a month. Her side, as they had been the some of her and a sering, and his and she had a most acting to be their\n",
      "presert of\n",
      "this way on an endeave to be surprised to\n",
      "the sister,” said her sister and she was somethand of all the reason of the country, and she was somether,\n",
      "and her four and\n",
      "heard\n",
      "was\n",
      "a marry as the place of any\n",
      "other say which he had not to be some on her assurion which he sat she could have been all the reason to his sister, and taken the sonter, that they walked to\n",
      "the motthe when they had not a sone that the\n",
      "present of her\n",
      "fair, and she was something of her acting a meater to be such as to brother, with all this so seemed as to\n",
      "be such a most some acceptance, and that she can have a latter of her sister, who had been a little assured her at a much of thim to be sure her fate will be thein astentions of the party. She was not belees that they was some acquainted as tell that she had no manner of a last sister and she had always and thanking to him. He h\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model = model, size = 1000,seed='The', k = 3)) #so taking the top 3 most likely choices for the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Shakespeare Text Generation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1605840538890,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "H8y9_3tKdb8y",
    "outputId": "2221afd2-0ccb-4587-9e07-fb35bc6df948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther shall not\n",
      "  And take his senseless fingers. What, are you?\n",
      "  OTHELLO. And what is that?\n",
      "  OTHELLO. It is the wind of mark himself a man,\n",
      "    As with the wint of such dispatch and things.\n",
      "    This is the world, and some such serpent strikes\n",
      "    The world to tender.\n",
      "  MONTANO. What, will you take him on,\n",
      "    Then that your sovereign lady? I had both  \n",
      "    This world the season without many treasons\n",
      "    And seem to strain how the stars of the storm\n",
      "    Whose third importunate and subjects' seas\n",
      "    They would have seen to hardly both of me.\n",
      "    To take him with the choice, and the fair lords.\n",
      "    We will be sent to send the world of heaven,\n",
      "    The stones of heart that with his brain and strength\n",
      "    They wished with a state. Though they would seem\n",
      "    And wishes with a first as starved with him;\n",
      "    And with a firm to see the shame thou hadst,\n",
      "    Wherein I would have stood out of the court,\n",
      "    And this, the secret shame, and that which shall\n",
      "    Where strike away the foulter of his so\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model = model, size = 1000,seed='The', k = 3)) #copied and pasted from Shakespeare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tom Sawyer Text Generation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1605840538890,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "H8y9_3tKdb8y",
    "outputId": "2221afd2-0ccb-4587-9e07-fb35bc6df948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The wishing a courte to he word. The beat till a streat on his some and the sand an it wonthing to the book the say and hass and to the sunters and they sone and had there was a come of him and her and then anythen the\n",
      "boy that this than treit and that way to\n",
      "the cound of a porenters, and she say has the sare they was a suppinted of\n",
      "her the hard to hear and\n",
      "town an all her and that time and his him to him as a chomess and hers of the boys, and him his hadner of\n",
      "the sured and\n",
      "the bears, and his his had said a strate of he with had and strong and then a shound a courted himself that the conden to the\n",
      "boys, and\n",
      "his say a pontles and here thit had some his wanted to stear than the shoring to\n",
      "had to the sand an the stracked to time the sardent and his winged a cross, and soment and to the sand, and soming.\n",
      "He his was that he would the strown the came and the crosting of a coull the sain of the his aroust as the said:\n",
      "\n",
      "“Oh, I'd seath on,”\n",
      "-they handsed a than the condient to to the said and\n",
      "say \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model = model, size = 1000,seed='The', k = 3)) #copied and pasted from Tom Sawyer results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**War and Peace Text Generation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There the same son’s army, as if he were still a moment of the carts of the countess,\n",
      "who had seen all the cheets that the conversation, as to the passage of his soul,\n",
      "before the countess, to the countess and a princess the\n",
      "passage.\n",
      "\n",
      "The storing of\n",
      "her hand, and the captain went out of the\n",
      "same son of that sound of the position, and a propose to the commander in chief of her\n",
      "son, the sound of the country. The prince had been too seeing them the\n",
      "sound of the campaign to to see the strange command of their presentes of horses at the soldious. The campaign. That he was so asking that the partical walking their honor of the same army. The conscalions, the partious and his hand, as he was so and that it was not a marriage that they was a contempt of\n",
      "the country as the strangers at the strange tones, the same time was still all all the property, the count and him to the countess, and the packet was a serfs to her any the position, was the part of that countess, and having some time at\n",
      "him.\n",
      "\n",
      "“The\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model = model, size = 1000,seed='The', k = 3)) #copied and pasted from War & Peace results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Meditation Text Generation Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 695,
     "status": "ok",
     "timestamp": 1605840538890,
     "user": {
      "displayName": "Hojin Mok",
      "photoUrl": "",
      "userId": "00800008926615240683"
     },
     "user_tz": 300
    },
    "id": "H8y9_3tKdb8y",
    "outputId": "2221afd2-0ccb-4587-9e07-fb35bc6df948"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The was trough of that\n",
      "is a porse to bo that\n",
      "is the wart and to the ware and a thing, thou dost not they shall thou may they are to the power things is it, as it thou dott neater and all to may the partion and thee and as the sane of in tree and such and sees, as the soun that which is a proses and to\n",
      "the part of and any to to\n",
      "that they shall as it to bo to be neither in the\n",
      "seese and somming antt theme of a man shill and,\n",
      "what as it is that what as it in the proper of in the pars on intended of the preselves and some, that whan thou deed in that is all that thou mane as it all time and all thas are subsion and to man to things ale as the panticire to to be neather of the world of the pract of\n",
      "the seant of a mitted the same of that are such and\n",
      "all things and thee wholly\n",
      "that who that the soul of in the same the part and that that is is intind of to the world the sumpensest\n",
      "and such of that wish any as in any things, and to theme any things it an one and to the would that whole the proper \n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model = model, size = 1000,seed='The', k = 3)) #copied and pasted from Meditation results"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyPpTk7wuSIblp0wFg3jX1TJ",
   "collapsed_sections": [],
   "mount_file_id": "19EqFEA44ckxWh71N61zVEIwDxSd3DOFa",
   "name": "NLP.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
